{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Introduction_to_Gradient_Descent.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FRbIcCiotoce"},"source":["<a name=\"TF-KS\"></a>\n","## **Introduction to Gradient Descent**"]},{"cell_type":"markdown","metadata":{"id":"mR78a4QIbEow"},"source":["<a name=\"Dataset\"></a>\n","## **Dataset**\n"]},{"cell_type":"markdown","metadata":{"id":"SGHwaOOxQXJe"},"source":["<h4>Used Cars</h4>\n","\n","<p><img alt=\"Colaboratory logo\" height=\"100px\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/GAZ-24_Volga.svg/640px-GAZ-24_Volga.svg.png\" align=\"center\" hspace=\"10px\" vspace=\"0px\"></p>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tVe2wKVGSwIV","outputId":"fd7ebc3b-614c-4095-f6a2-0f0c3d1a2638"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"59_A44_uS6PP","outputId":"61fc0a30-12bc-46b3-aa59-172b64af1768"},"source":["import pandas as pd\n","data = pd.read_csv(\"/content/drive/My Drive/ann_used_cars.csv\", usecols=[\"mileage\", \"price\"])\n","data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mileage</th>\n","      <th>price</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7413</td>\n","      <td>21992</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10926</td>\n","      <td>20995</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>7351</td>\n","      <td>19995</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11613</td>\n","      <td>17809</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>8367</td>\n","      <td>17500</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   mileage  price\n","0     7413  21992\n","1    10926  20995\n","2     7351  19995\n","3    11613  17809\n","4     8367  17500"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"xm04xHez9rF_"},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(data['mileage'], data['price'], test_size=0.3, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6iFUBMV4EpY2"},"source":["# Normalizing\n","import numpy as np\n","X_train = np.atleast_2d(X_train).T\n","from sklearn import preprocessing\n","scaler = preprocessing.MinMaxScaler()\n","X_train = scaler.fit_transform(X_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZgIdr3eivVqA"},"source":["# Convert dataset to tensors\n","import tensorflow as tf\n","\n","X_train=tf.convert_to_tensor(X_train, dtype= tf.float32)\n","y_train=tf.convert_to_tensor(y_train, dtype= tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qbwcucURbJRM"},"source":["<a name=\"Training\"></a>\n","## **Gradient Descent**\n"]},{"cell_type":"markdown","metadata":{"id":"bI_R8pu8SUVf"},"source":["<a href=\"https://ibb.co/qntkSS8\"><img src=\"https://i.ibb.co/n0HzGGy/gd.png\" height=\"250px\" alt=\"gd\" border=\"0\"></a>"]},{"cell_type":"markdown","metadata":{"id":"IC2dGa8ZTGlD"},"source":["### **Step 1: Start at a random bias and weight**\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"wc0dUc_InFPl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eb3d7099-ad15-47dd-e2da-d4c6704b31f2"},"source":["input_dim = X_train.shape[1]\n","output_dim = 1\n","learning_rate = 0.01\n","\n","weight = tf.Variable(tf.random.normal([input_dim, output_dim])) \n","bias = tf.Variable(tf.random.normal([output_dim,])) \n","print(weight)\n","print(bias)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[0.19899713]], dtype=float32)>\n","<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.39151314], dtype=float32)>\n"]}]},{"cell_type":"markdown","metadata":{"id":"c8K0KZiVfjji"},"source":["### **Step 2: Take a step in the direction with the steepest gradient**\n","### **Step 3: Calculate the new loss**\n","### **Step 4: Repeat steps 2 and 3**\n","\n"]},{"cell_type":"code","metadata":{"id":"njL9lX96nQtG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8ae2ba82-630e-4c0e-dddd-bca4119afab8"},"source":["for steps in range(100):\n","\n","  with tf.GradientTape() as tape:\n","    \n","   # Predict\n","   predictions = tf.matmul(X_train, weight) + bias\n","\n","   # Calculate loss\n","   loss = tf.reduce_mean(tf.square(y_train - predictions)) # mean square error   \n","   dloss_dw, dloss_db = tape.gradient(loss, [weight, bias]) \n","\n","  weight.assign_sub(learning_rate * dloss_dw)\n","  bias.assign_sub(learning_rate * dloss_db)\n","\n","  print('Step %d: Weight:%1d Bias:%d Loss = %.0f' % (steps+1, weight, bias, loss))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Step 1: Weight:67 Bias:257 Loss = 175150624\n","Step 2: Weight:132 Bias:509 Loss = 168152592\n","Step 3: Weight:196 Bias:756 Loss = 161450800\n","Step 4: Weight:259 Bias:997 Loss = 155032704\n","Step 5: Weight:320 Bias:1233 Loss = 148886384\n","Step 6: Weight:380 Bias:1464 Loss = 143000224\n","Step 7: Weight:439 Bias:1690 Loss = 137363296\n","Step 8: Weight:496 Bias:1912 Loss = 131965000\n","Step 9: Weight:552 Bias:2128 Loss = 126795224\n","Step 10: Weight:607 Bias:2340 Loss = 121844344\n","Step 11: Weight:660 Bias:2548 Loss = 117103040\n","Step 12: Weight:712 Bias:2751 Loss = 112562472\n","Step 13: Weight:764 Bias:2949 Loss = 108214112\n","Step 14: Weight:814 Bias:3144 Loss = 104049848\n","Step 15: Weight:863 Bias:3334 Loss = 100061864\n","Step 16: Weight:911 Bias:3520 Loss = 96242720\n","Step 17: Weight:957 Bias:3702 Loss = 92585248\n","Step 18: Weight:1003 Bias:3881 Loss = 89082624\n","Step 19: Weight:1048 Bias:4055 Loss = 85728272\n","Step 20: Weight:1092 Bias:4226 Loss = 82515920\n","Step 21: Weight:1134 Bias:4393 Loss = 79439552\n","Step 22: Weight:1176 Bias:4557 Loss = 76493432\n","Step 23: Weight:1217 Bias:4717 Loss = 73672024\n","Step 24: Weight:1257 Bias:4874 Loss = 70970040\n","Step 25: Weight:1296 Bias:5027 Loss = 68382440\n","Step 26: Weight:1334 Bias:5177 Loss = 65904380\n","Step 27: Weight:1371 Bias:5324 Loss = 63531212\n","Step 28: Weight:1408 Bias:5468 Loss = 61258524\n","Step 29: Weight:1443 Bias:5609 Loss = 59082028\n","Step 30: Weight:1478 Bias:5746 Loss = 56997668\n","Step 31: Weight:1512 Bias:5881 Loss = 55001532\n","Step 32: Weight:1546 Bias:6013 Loss = 53089904\n","Step 33: Weight:1578 Bias:6142 Loss = 51259184\n","Step 34: Weight:1610 Bias:6269 Loss = 49505964\n","Step 35: Weight:1641 Bias:6392 Loss = 47826956\n","Step 36: Weight:1671 Bias:6513 Loss = 46219012\n","Step 37: Weight:1701 Bias:6632 Loss = 44679128\n","Step 38: Weight:1730 Bias:6748 Loss = 43204428\n","Step 39: Weight:1758 Bias:6861 Loss = 41792144\n","Step 40: Weight:1786 Bias:6972 Loss = 40439640\n","Step 41: Weight:1813 Bias:7081 Loss = 39144376\n","Step 42: Weight:1839 Bias:7187 Loss = 37903936\n","Step 43: Weight:1865 Bias:7291 Loss = 36716000\n","Step 44: Weight:1890 Bias:7393 Loss = 35578340\n","Step 45: Weight:1915 Bias:7493 Loss = 34488828\n","Step 46: Weight:1939 Bias:7590 Loss = 33445432\n","Step 47: Weight:1963 Bias:7686 Loss = 32446188\n","Step 48: Weight:1986 Bias:7779 Loss = 31489236\n","Step 49: Weight:2008 Bias:7871 Loss = 30572784\n","Step 50: Weight:2030 Bias:7960 Loss = 29695114\n","Step 51: Weight:2051 Bias:8048 Loss = 28854588\n","Step 52: Weight:2072 Bias:8134 Loss = 28049628\n","Step 53: Weight:2093 Bias:8218 Loss = 27278734\n","Step 54: Weight:2113 Bias:8300 Loss = 26540464\n","Step 55: Weight:2132 Bias:8380 Loss = 25833430\n","Step 56: Weight:2151 Bias:8459 Loss = 25156310\n","Step 57: Weight:2170 Bias:8536 Loss = 24507852\n","Step 58: Weight:2188 Bias:8611 Loss = 23886826\n","Step 59: Weight:2206 Bias:8685 Loss = 23292074\n","Step 60: Weight:2223 Bias:8757 Loss = 22722482\n","Step 61: Weight:2240 Bias:8828 Loss = 22176996\n","Step 62: Weight:2256 Bias:8897 Loss = 21654584\n","Step 63: Weight:2272 Bias:8965 Loss = 21154268\n","Step 64: Weight:2288 Bias:9031 Loss = 20675124\n","Step 65: Weight:2303 Bias:9096 Loss = 20216248\n","Step 66: Weight:2318 Bias:9159 Loss = 19776780\n","Step 67: Weight:2333 Bias:9222 Loss = 19355904\n","Step 68: Weight:2347 Bias:9282 Loss = 18952826\n","Step 69: Weight:2361 Bias:9342 Loss = 18566804\n","Step 70: Weight:2374 Bias:9400 Loss = 18197110\n","Step 71: Weight:2388 Bias:9457 Loss = 17843042\n","Step 72: Weight:2400 Bias:9513 Loss = 17503952\n","Step 73: Weight:2413 Bias:9568 Loss = 17179202\n","Step 74: Weight:2425 Bias:9621 Loss = 16868182\n","Step 75: Weight:2437 Bias:9674 Loss = 16570314\n","Step 76: Weight:2449 Bias:9725 Loss = 16285042\n","Step 77: Weight:2460 Bias:9775 Loss = 16011833\n","Step 78: Weight:2471 Bias:9824 Loss = 15750167\n","Step 79: Weight:2482 Bias:9872 Loss = 15499570\n","Step 80: Weight:2493 Bias:9919 Loss = 15259566\n","Step 81: Weight:2503 Bias:9965 Loss = 15029702\n","Step 82: Weight:2513 Bias:10010 Loss = 14809555\n","Step 83: Weight:2523 Bias:10054 Loss = 14598713\n","Step 84: Weight:2532 Bias:10098 Loss = 14396778\n","Step 85: Weight:2541 Bias:10140 Loss = 14203380\n","Step 86: Weight:2551 Bias:10181 Loss = 14018150\n","Step 87: Weight:2559 Bias:10222 Loss = 13840745\n","Step 88: Weight:2568 Bias:10261 Loss = 13670833\n","Step 89: Weight:2576 Bias:10300 Loss = 13508103\n","Step 90: Weight:2584 Bias:10338 Loss = 13352242\n","Step 91: Weight:2592 Bias:10375 Loss = 13202966\n","Step 92: Weight:2600 Bias:10412 Loss = 13059990\n","Step 93: Weight:2607 Bias:10447 Loss = 12923051\n","Step 94: Weight:2615 Bias:10482 Loss = 12791899\n","Step 95: Weight:2622 Bias:10516 Loss = 12666276\n","Step 96: Weight:2629 Bias:10550 Loss = 12545955\n","Step 97: Weight:2635 Bias:10583 Loss = 12430714\n","Step 98: Weight:2642 Bias:10615 Loss = 12320337\n","Step 99: Weight:2648 Bias:10646 Loss = 12214618\n","Step 100: Weight:2654 Bias:10677 Loss = 12113355\n"]}]},{"cell_type":"code","metadata":{"id":"HTTqfSLlkzDw"},"source":[""],"execution_count":null,"outputs":[]}]}