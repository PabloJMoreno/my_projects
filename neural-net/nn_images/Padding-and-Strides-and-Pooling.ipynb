{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6. Padding, Strides and Pooling.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Padding, Strides, and Pooling**\n"],"metadata":{"id":"8XysHiAdX-RE"}},{"cell_type":"markdown","source":["Some of the major ideas we will try to understand through this notebook:\n","\n","*   The **working of a convolutional layer**\n","*   The **impact of filters on the output shape**\n","*   The **use of padding** to overcome the problem of under-utilizing pixels at the edge and the shrinking effect of convolutions\n","*   The **use of strides**\n","*   The **use of pooling** as a robust method for eliminating unwanted features\n","\n","\n","\n"],"metadata":{"id":"adebWtXGf3l1"}},{"cell_type":"markdown","source":["\n","* In Convolutional Neural Networks (CNNs), we require modified convolution layers to apply additional operations like padding, strides and pooling on an input.\n","* The input may be a 2D or 3D image, but typically it is a 3D image with rows, columns and channels as input. So the filters should also be in three dimensions, as they need to have an equal number of channels, even though they may have a smaller number of rows and columns.\n","* These filters essentially operate over the input, and generate output feature maps, which are arrays or matrices as well.\n"],"metadata":{"id":"1Lw3uTgTUfa0"}},{"cell_type":"markdown","source":["## **Introduction** "],"metadata":{"id":"UiLsqa2pXVid"}},{"cell_type":"markdown","source":["For the purpose of this exercise, we will use a simple input image sample. \n","\n","This 8x8 square image has only one channel, with a 2 pixel-wide vertical line of 1 over the center and with all the other values being 0. \n","\n","This input can be depicted as a matrix shown in the image below <br><br>\n","![Input.png](data:image/png;base64,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)\n"],"metadata":{"id":"xE-CzgeLohB6"}},{"cell_type":"code","source":["# The below code can be used to ignore the warnings that may occur due to deprecations\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# This 2D input data can be created using the numpy array and then reshaped into 8x8x1 \n","from numpy import asarray\n","# Defining input data as a 2d list\n","df = [[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0]]\n","\n","# Converting the 2D list into a numpy array\n","df = asarray(df)\n","\n","# Reshaping the input array into 8x8x1\n","df = df.reshape(1, 8, 8, 1)  # (samples,rows,columns,depth/channels)\n"],"metadata":{"id":"br8AAy8_y2sn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note**: Here we have used the **reshape (1,8,8,1)** function with the 4 values being - **(samples, rows, columns, depth/channels)** \n","\n","*   Samples denote the **number of images**\n","*   Rows denote the **number of rows**\n","*   Columns denote the **number of columns**\n","*   Depth denotes the **number of channels**\n","\n"],"metadata":{"id":"NG_hrhQz3Y9H"}},{"cell_type":"markdown","source":["**We would also use a filter of size 3x3**. The filter would contain the weights that will be learned during the training process. These filters tell us about the features that it would detect in the input.<br>\n","\n","Keras provides a simple way of implementing a convolution layer, called **Conv2D**.<br>\n","\n","The Conv2D function is required to have the following parameters specified:\n","\n","*   **The input shape** of the input image\n","*   **The number of filters**\n","*   **The input shape of the filter**\n"],"metadata":{"id":"r8nxc3nJyc-C"}},{"cell_type":"markdown","source":["Here, we shall use 1 filter with a 3x3 filter-size and an input-size of (8,8,1) \n","\n","The link to the documentation of **[Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/)** can be looked over to learn about all the parameters in the function."],"metadata":{"id":"DGeVYt8A8piF"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OL2ExbDcWuTD","executionInfo":{"status":"ok","timestamp":1646396666383,"user_tz":-330,"elapsed":16,"user":{"displayName":"G Yeshwant Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04985776999194336326"}},"outputId":"c2eaee9f-9693-45d1-a435-24dd99f2ecb2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_21\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_29 (Conv2D)          (None, 6, 6, 1)           10        \n","                                                                 \n","=================================================================\n","Total params: 10\n","Trainable params: 10\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Here we will be using the sequential model and creating a convolutional layer using Conv2D\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D\n","\n","# Creating the sequential model with one convolutional layer\n","model = Sequential()\n","model.add(Conv2D(1, (3,3), input_shape=(8, 8, 1)))\n","# Model summary\n","model.summary()"]},{"cell_type":"markdown","source":["**The Conv2D method** is given with 4 inputs as shown in the above code where **1** represents the number of filters, **(3,3)** represents the shape of the filter and **input_shape=(8,8,1)** shows us the input shape that is given to this first convolution layer. <br>\n","\n","Currently the filter is intialized with random weights by default, but we will hard code our 3x3 filter weights for the time being to detect the vertical edges from our input.<br>\n","\n","**These filter weights would get strongly activated when they come across a vertical edge in the input**, otherwise they would be weakly activated. By \"activated\", we mean that **the output values for that region would be very high** in comparison, due to the nature of the sum of element-wise products for similar elements from the input and the filter. \n","\n","We are trying to use this filter over the input image to check if a vertical edge gets detected.\n"],"metadata":{"id":"qGQ5dxsa24OP"}},{"cell_type":"code","source":["import pandas as pd\n","# Defining a 3x3 vertical line detector \n","detector = [[[[0]],[[1]],[[0]]],\n","            [[[0]],[[1]],[[0]]],\n","            [[[0]],[[1]],[[0]]]]\n","pd.DataFrame(detector).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fh6AagPQdZLF","executionInfo":{"status":"ok","timestamp":1646396666384,"user_tz":-330,"elapsed":10,"user":{"displayName":"G Yeshwant Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04985776999194336326"}},"outputId":"150ae85a-074b-4394-beb9-e2e5277df5f2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 3)"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["The 3x3 vertical detector defined would be converted into a Numpy array and a bias value would be added to it. \n","\n","We then use the **set_weights()** function to set the weights to the model as shown below."],"metadata":{"id":"Hm-DLQHE79U-"}},{"cell_type":"code","source":["# Defining the weights using the detector and adding a bias value to it\n","weights = [asarray(detector), asarray([0.0])]\n","# Storing the weights in the model\n","model.set_weights(weights)"],"metadata":{"id":"ARQ_WeUH6W0k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Applying the filter over the input image using the **predict()** function"],"metadata":{"id":"15qWalNvFUKy"}},{"cell_type":"code","source":["# Using the filter on our input\n","y_hat = model.predict(df)\n","y_hat.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l9bWn5FFEdi-","executionInfo":{"status":"ok","timestamp":1646396667172,"user_tz":-330,"elapsed":46,"user":{"displayName":"G Yeshwant Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04985776999194336326"}},"outputId":"cab9fa54-30e9-47e4-e150-2447e66d4792"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 6, 6, 1)"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["We can observe that we have received a 4-dimensional output after using the predict function. This has four values **(batch/number of samples,rows,columns,number of filters)**.\n","\n","**Note**: There should be no confusion between the input shape to the first convolution layer and the output shape of the first convolution layer.<br>\n","\n","\n","*   The input shape to the first convolution layer should be **(batch/number of samples, rows,columns,depth/channels)**\n","*   The output shape to the first convolution layer should be **(batch/number of samples, rows,columns,number of filters in the convolution layer)**\n","\n","\n"],"metadata":{"id":"4FCW8w6GFv1E"}},{"cell_type":"code","source":["y_hat.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jylsFkMHKL9e","executionInfo":{"status":"ok","timestamp":1646396667172,"user_tz":-330,"elapsed":44,"user":{"displayName":"G Yeshwant Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04985776999194336326"}},"outputId":"b40ce525-2f8e-478a-d66c-d9f412ef08db"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 6, 6, 1)"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["for r in range(y_hat.shape[1]):     # Here y_hat.shape[1] will give us the row value as 6 in this example\n","\t# Printing each column in the row\n","  feature_map=[y_hat[0,r,c,0] for c in range(y_hat.shape[2])]  # Here y_hat.shape[1] will give us the column value as 6 in this example\n","  print(feature_map)\n","  # Here y_hat[0,r,c,0] signifies the first sample represented as 0 and the all the rows and columns as r and c and the filter which is indexed at 0 here."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vbk2tE25Fois","executionInfo":{"status":"ok","timestamp":1646396667173,"user_tz":-330,"elapsed":43,"user":{"displayName":"G Yeshwant Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04985776999194336326"}},"outputId":"67948e4d-4c5b-46af-9704-4f1baa7e83ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n","[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n","[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n","[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n","[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n","[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n"]}]},{"cell_type":"markdown","source":["* We can observe that **we are extracting the output feature map**, and that **the vertical edges** from the middle of the input **are detected** using this filter.\n","*   We can also observe that we have provided an input image of 8x8, but **we have received an output of 6x6.** This shows that we got the output reduced from 64 pixels to 36 pixels when we used a filter of 3x3. so we have lost 28 pixels here. This effect of reduction of the size of input, is called the **Border Effect** of convolutions.\n","*  This is not much of a problem when we have images of a larger size and filters of comparatively much smaller sizes, but it will become a problem with smaller images, as we may lose an important amount of information from the input data.\n","*  This **Border Effect** also increases with an increase in the number of convolution layers stacked one after another. \n","<br> For example, if we were to add one more convolution layer with a filter of 3x3 after this example, then the 8x8 input image would eventually reduce to 4x4.  \n","\n","\n","\n","\n","\n"],"metadata":{"id":"Mk_WjrW1Md0o"}},{"cell_type":"code","source":["model = Sequential()\n","model.add(Conv2D(1, (3,3), input_shape=(8, 8, 1)))\n","model.add(Conv2D(1, (3,3)))\n","# Model summary\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"19tDBYjPyBKA","executionInfo":{"status":"ok","timestamp":1646396667173,"user_tz":-330,"elapsed":40,"user":{"displayName":"G Yeshwant Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04985776999194336326"}},"outputId":"bfbf8812-7856-424e-89ec-1c29835fdcff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_22\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_30 (Conv2D)          (None, 6, 6, 1)           10        \n","                                                                 \n"," conv2d_31 (Conv2D)          (None, 4, 4, 1)           10        \n","                                                                 \n","=================================================================\n","Total params: 20\n","Trainable params: 20\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["*  The Border Effect can hence not just be left unchecked. We need a control, that determines whether we want to allow the Border Effect or not, and if not, we want to obtain an output with the same size as that of the input. <br><br> \n","This can be done by adding a variation to convolution, called **Padding**."],"metadata":{"id":"2OCyf8WdyBd4"}},{"cell_type":"markdown","source":["## **Padding**\n","\n","\n"],"metadata":{"id":"xspye1k5pyM8"}},{"cell_type":"markdown","source":["*   **In Padding, we add pixels to the border of the input image by \"padding\" it with zeros**.\n","*   In TensorFlow and Keras, padding is implemented using a **padding** argument in the Conv2D layer. \n","* There are two values which can be used with padding - **valid** which means there is no padding and **same** which adds padding as required to the input image and ensures that the output has the same shape as that of the input."],"metadata":{"id":"W10JlriH5eZM"}},{"cell_type":"code","source":["# Padding can be implemented as shown below\n","# Creating a sequential model with padding ='same'\n","model = Sequential()\n","model.add(Conv2D(1, (3,3), padding='same', input_shape=(8, 8, 1)))\n","# Model summary\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fvYlheV4KExd","executionInfo":{"status":"ok","timestamp":1646396667174,"user_tz":-330,"elapsed":35,"user":{"displayName":"G Yeshwant Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04985776999194336326"}},"outputId":"b1d17397-6740-4f5f-e89c-fcd392234dec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_23\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_32 (Conv2D)          (None, 8, 8, 1)           10        \n","                                                                 \n","=================================================================\n","Total params: 10\n","Trainable params: 10\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["*   We observe that with **'same' padding**, the output shape of the image is same as input with shape=8x8\n","*   Thus **Same Padding helps in keepiing the output shape the same as the input.** This is true for any number of Conv2D layers that we add with padding = 'same', **the shape always remains the same**, as shown below.\n","\n"],"metadata":{"id":"ijcraGC5JR5A"}},{"cell_type":"code","source":["# Creating a sequential model with 4 layers and padding='same'\n","model = Sequential()\n","model.add(Conv2D(1, (3,3), padding='same', input_shape=(8, 8, 1)))\n","model.add(Conv2D(1, (3,3), padding='same'))\n","model.add(Conv2D(1, (3,3), padding='same'))\n","model.add(Conv2D(1, (3,3), padding='same'))\n","# Model summary\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k-h5ZpHnNqru","executionInfo":{"status":"ok","timestamp":1646396667174,"user_tz":-330,"elapsed":27,"user":{"displayName":"G Yeshwant Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04985776999194336326"}},"outputId":"ff7139d2-628a-432a-df68-6a42c7033965"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_24\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_33 (Conv2D)          (None, 8, 8, 1)           10        \n","                                                                 \n"," conv2d_34 (Conv2D)          (None, 8, 8, 1)           10        \n","                                                                 \n"," conv2d_35 (Conv2D)          (None, 8, 8, 1)           10        \n","                                                                 \n"," conv2d_36 (Conv2D)          (None, 8, 8, 1)           10        \n","                                                                 \n","=================================================================\n","Total params: 40\n","Trainable params: 40\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["As observed, the output shape remains the same as the input shape."],"metadata":{"id":"h4wnI6i5MXBL"}},{"cell_type":"markdown","source":["## **Strides**"],"metadata":{"id":"7nfxv6iQWPM4"}},{"cell_type":"markdown","source":["*   Strides are used for reducing the dimensionality of the input, and making the convolution operation invariant to minor changes like roation and shifting. It modifies the convolution over the input, to now using a filter **with a particular shift value, known as the stride.**\n","*   In TensorFlow and Keras, Stride is implemented using a **stride** arguement in the Conv2D layer. **It should be specified as a tuple, with a width and height value.** \n","* Here, we will be using a stride of (2,2).\n"],"metadata":{"id":"iWHX8yxPXbUx"}},{"cell_type":"code","source":["# Creating a sequential model with a filter of 3x3 and using stride (2,2) over an 8x8 input image\n","model = Sequential()\n","model.add(Conv2D(1, (3,3), strides=(2, 2), input_shape=(8, 8, 1)))\n","# Model summary\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4czREOWLWRCf","executionInfo":{"status":"ok","timestamp":1646396667175,"user_tz":-330,"elapsed":20,"user":{"displayName":"G Yeshwant Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04985776999194336326"}},"outputId":"cd47d495-3a23-46cf-aa75-54e7ae9c161b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_25\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_37 (Conv2D)          (None, 3, 3, 1)           10        \n","                                                                 \n","=================================================================\n","Total params: 10\n","Trainable params: 10\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["As we observe, the output shape has now decreased. \n","\n","Now, we will also use this model on the same input data of 8x8 pixels and the same vertical detector/filter of 3x3."],"metadata":{"id":"kBeUEwL2bS85"}},{"cell_type":"code","source":["y_hat = model.predict(df)\n","\n","for r in range(y_hat.shape[1]):     # Here y_hat.shape[1] will give us the row value as 6 in this example\n","\t# Printing each column in the row\n","  feature_map=[y_hat[0,r,c,0] for c in range(y_hat.shape[2])]  # Here y_hat.shape[1] will give us the column value as 6 in this example\n","  print(feature_map)\n","  # Here y_hat[0,r,c,0] signifies the first sample represented as 0 and the all the rows and columns as r and c and the filter which is indexed at 0 here."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BU78ZbujbOcc","executionInfo":{"status":"ok","timestamp":1646396667176,"user_tz":-330,"elapsed":16,"user":{"displayName":"G Yeshwant Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04985776999194336326"}},"outputId":"2b8a0073-93fb-419d-9662-a89bbd213484"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.0, -0.72713476, 0.40972036]\n","[0.0, -0.72713476, 0.40972036]\n","[0.0, -0.72713476, 0.40972036]\n"]}]},{"cell_type":"code","source":["# Example of vertical line filter with a stride of 2\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D\n","from numpy import asarray\n","\n","# This 2d input data can be created using the numpy array and then reshaped into 8x8x1 \n","# Defining input data as a 2d list\n","df = [[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0]]\n","# Converting the 2d list into numpy array\n","df = asarray(df)\n","# Reshaping the one input array into 8x8x1\n","df = df.reshape(1, 8, 8, 1)  # (samples,rows,columns,depth/channels)\n","\n","# Creating a sequential model with a filter of 3x3 and using stride (2,2) over an input image of 8x8 pixel values\n","model = Sequential()\n","model.add(Conv2D(1, (3,3), strides=(2, 2), input_shape=(8, 8, 1)))\n","# model summary\n","model.summary()\n","\n","# Defining a 3x3 vertical line detector \n","detector = [[[[0]],[[1]],[[0]]],\n","            [[[0]],[[1]],[[0]]],\n","            [[[0]],[[1]],[[0]]]]\n","# Defining the weights using the detector and adding a bias value to it\n","weights = [asarray(detector), asarray([0.0])]\n","# Stroing the weights in the model\n","model.set_weights(weights)\n","\n","# Using the filter on our input\n","y_hat = model.predict(df)\n","\n","for r in range(y_hat.shape[1]):     # Here y_hat.shape[1] will give us the row value as 6 in this example\n","\t# Printing each column in the row\n","  feature_map=[y_hat[0,r,c,0] for c in range(y_hat.shape[2])]  # Here y_hat.shape[1] will give us the column value as 6 in this example\n","  print(feature_map)\n","  # Here y_hat[0,r,c,0] signifies the first sample represented as 0 and the all the rows and columns as r and c and the filter which is indexed at 0 here.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SNvOnAd3cnZS","executionInfo":{"status":"ok","timestamp":1646396668079,"user_tz":-330,"elapsed":918,"user":{"displayName":"G Yeshwant Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04985776999194336326"}},"outputId":"b77766ef-5258-431c-b169-5c8058554380"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_26\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_38 (Conv2D)          (None, 3, 3, 1)           10        \n","                                                                 \n","=================================================================\n","Total params: 10\n","Trainable params: 10\n","Non-trainable params: 0\n","_________________________________________________________________\n","[0.0, 3.0, 0.0]\n","[0.0, 3.0, 0.0]\n","[0.0, 3.0, 0.0]\n"]}]},{"cell_type":"markdown","source":["We observe that the input has been downsampled and the input dimension has been reduced. \n","\n","However we can still detect the vertical line in the input and represent this vertical edge with less information."],"metadata":{"id":"T9xdxt6sekWX"}},{"cell_type":"markdown","source":["## **Pooling**"],"metadata":{"id":"XvTAayebfLMs"}},{"cell_type":"markdown","source":["### **The ReLU Activation Function before Pooling**"],"metadata":{"id":"zHpHEMr6YR77"}},{"cell_type":"markdown","source":["We have already looked into an example showing convolution filters, padding and strides. \n","\n","Before going over the idea of pooling, we will look into an example where the **ReLU** activation function is applied.\n","\n","*  The **ReLU or Rectified Linear Unit** activation function, is first applied over each value of the obtained output feature map (after the convolution, padding and stride operations), since it will contain negative values on account of the negative values present in the filter.\n","* This is an effective way to **add non-linearity to the model** without changing the values of the feature map.\n","*  **Adding such non-linearity using ReLU is considered a best practice** in Computer Vision research, **before adding pooling to our model.**\n","\n","\n","\n"],"metadata":{"id":"z5YXfYHzmLVW"}},{"cell_type":"code","source":["# Example showing the use of Activation function ReLU in our model\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D\n","from numpy import asarray\n","\n","# This 2d input data can be created using the numpy array and then reshaped into 8x8x1 \n","# Defining input data as a 2d list\n","df = [[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0]]\n","# Converting the 2d list into numpy array\n","df = asarray(df)\n","# Reshaping the one input array into 8x8x1\n","df = df.reshape(1, 8, 8, 1)  # (samples,rows,columns,depth/channels)\n","\n","# Creating a sequential model with a filter of 3x3 with relu activation over an input image of 8x8 pixel values\n","model = Sequential()\n","model.add(Conv2D(1, (3,3), activation='relu', input_shape=(8, 8, 1)))\n","# model summary\n","model.summary()\n","\n","# Defining a 3x3 vertical line detector \n","detector = [[[[0]],[[1]],[[0]]],\n","            [[[0]],[[1]],[[0]]],\n","            [[[0]],[[1]],[[0]]]]\n","# Defining the weights using the detector and adding a bias value to it\n","weights = [asarray(detector), asarray([0.0])]\n","# Stroing the weights in the model\n","model.set_weights(weights)\n","\n","# Using the filter on our input\n","y_hat = model.predict(df)\n","\n","for r in range(y_hat.shape[1]):     # Here y_hat.shape[1] will give us the row value as 6 in this example\n","\t# Printing each column in the row\n","  feature_map=[y_hat[0,r,c,0] for c in range(y_hat.shape[2])]  # Here y_hat.shape[1] will give us the column value as 6 in this example\n","  print(feature_map)\n","  # Here y_hat[0,r,c,0] signifies the first sample represented as 0 and the all the rows and columns as r and c and the filter which is indexed at 0 here.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aG-LDRJUmKl1","executionInfo":{"status":"ok","timestamp":1646396668080,"user_tz":-330,"elapsed":45,"user":{"displayName":"G Yeshwant Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04985776999194336326"}},"outputId":"4d8ed7e2-4354-429c-af2c-a638431a4b0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_27\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_39 (Conv2D)          (None, 6, 6, 1)           10        \n","                                                                 \n","=================================================================\n","Total params: 10\n","Trainable params: 10\n","Non-trainable params: 0\n","_________________________________________________________________\n","[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n","[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n","[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n","[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n","[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n","[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n"]}]},{"cell_type":"markdown","source":["We observe that the vertical edge has been detected."],"metadata":{"id":"hOGed1bZow0h"}},{"cell_type":"markdown","source":["### **Average Pooling**"],"metadata":{"id":"9nuBAtm7lnmZ"}},{"cell_type":"markdown","source":["*   **Average Pooling** is applied over a pool size / patch size of 2x2, and **to obtain the average value** of the values in that given patch size.\n","*   Thus the 2x2 size feature map gets downsampled to its regional average values.\n","*   Average Pooling can be implemented in Keras using the **AveragePooling2D** layer. This is imported from **tensorflow.keras.layers**.\n","*   The default pool size is (2,2), and the stride vaue in this case means using the pool size as the stride value (2,2).\n","\n"],"metadata":{"id":"BZNY3U-UpdOM"}},{"cell_type":"code","source":["# Example showing the use of Activation function Relu and AveragePooling2D() layer\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, AveragePooling2D\n","from numpy import asarray\n","\n","# This 2d input data can be created using the numpy array and then reshaped into 8x8x1 \n","# Defining input data as a 2d list\n","df = [[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n","\t\t[0, 0, 0, 1, 1, 0, 0, 0]]\n","# Converting the 2d list into numpy array\n","df = asarray(df)\n","# Reshaping the one input array into 8x8x1\n","df = df.reshape(1, 8, 8, 1)  # (samples,rows,columns,depth/channels)\n","\n","# Creating a sequential model with a filter of 3x3 with relu activation over an input image of 8x8 pixel values and adding a pooling layer\n","model = Sequential()\n","model.add(Conv2D(1, (3,3), activation='relu', input_shape=(8, 8, 1)))\n","model.add(AveragePooling2D())\n","# model summary\n","model.summary()\n","\n","# Defining a 3x3 vertical line detector \n","detector = [[[[0]],[[1]],[[0]]],\n","            [[[0]],[[1]],[[0]]],\n","            [[[0]],[[1]],[[0]]]]\n","# Defining the weights using the detector and adding a bias value to it\n","weights = [asarray(detector), asarray([0.0])]\n","# Stroing the weights in the model\n","model.set_weights(weights)\n","\n","# Using the filter on our input\n","y_hat = model.predict(df)\n","\n","for r in range(y_hat.shape[1]):     # Here y_hat.shape[1] will give us the row value as 6 in this example\n","\t# Printing each column in the row\n","  feature_map=[y_hat[0,r,c,0] for c in range(y_hat.shape[2])]  # Here y_hat.shape[1] will give us the column value as 6 in this example\n","  print(feature_map)\n","  # Here y_hat[0,r,c,0] signifies the first sample represented as 0 and the all the rows and columns as r and c and the filter which is indexed at 0 here.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sqjbUsSFcxdo","executionInfo":{"status":"ok","timestamp":1646396668080,"user_tz":-330,"elapsed":34,"user":{"displayName":"G Yeshwant Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04985776999194336326"}},"outputId":"bd39d536-dd76-407e-b07d-9cb82e148de2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_28\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_40 (Conv2D)          (None, 6, 6, 1)           10        \n","                                                                 \n"," average_pooling2d_1 (Averag  (None, 3, 3, 1)          0         \n"," ePooling2D)                                                     \n","                                                                 \n","=================================================================\n","Total params: 10\n","Trainable params: 10\n","Non-trainable params: 0\n","_________________________________________________________________\n","[0.0, 3.0, 0.0]\n","[0.0, 3.0, 0.0]\n","[0.0, 3.0, 0.0]\n"]}]},{"cell_type":"markdown","source":["We observe that the vertical edge has been detected even after downsampling the feature map."],"metadata":{"id":"vbxhXKSbtXyl"}},{"cell_type":"markdown","source":["### **Max Pooling**"],"metadata":{"id":"DY3Aj8T-vUZQ"}},{"cell_type":"markdown","source":["*   **Max Pooling** is similar to Average Pooling, but **it uses the maximum value over the patch size** rather than the average value.\n","*   It is implemented in Keras using the **MaxPooling2D** layer. This also needs to be imported from **tensorflow.keras.layers**."],"metadata":{"id":"PmX_cdebvYA0"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Conv2D, AveragePooling2D, MaxPooling2D\n","# Create a model with one convolutional layer and max pooling layer\n","model = Sequential()\n","model.add(Conv2D(1, (3,3), activation='relu', input_shape=(8, 8, 1)))\n","model.add(MaxPooling2D())\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tMOzsDictROK","executionInfo":{"status":"ok","timestamp":1646396668081,"user_tz":-330,"elapsed":21,"user":{"displayName":"G Yeshwant Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04985776999194336326"}},"outputId":"86075b55-fb0b-48c7-a13e-28000cb00d4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_29\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_41 (Conv2D)          (None, 6, 6, 1)           10        \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 3, 3, 1)          0         \n"," 2D)                                                             \n","                                                                 \n","=================================================================\n","Total params: 10\n","Trainable params: 10\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["We observe that the feature maps have been decreased to half after adding the max_pooling layer over the convolution layer."],"metadata":{"id":"UZMme0cGwX4I"}},{"cell_type":"markdown","source":["# **Happy Learning!**"],"metadata":{"id":"WvqIQfsC4n6W"}}]}